{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=P6sfmUTpUmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open(\"names.txt\").read().splitlines()\n",
    "print(f\"{len(words)} examples: {words[:8]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocabulary of characters and mappings\n",
    "chars = [\".\"] + sorted(list(set(\"\".join(words))))\n",
    "s2i = {s: i for i, s in enumerate(chars)}\n",
    "i2s = {i: s for i, s in enumerate(chars)}\n",
    "print(f\"Vocab size: {len(i2s)}  \\n{i2s}\")\n",
    "vocab_size = len(i2s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + \".\":\n",
    "            idx = s2i[ch]\n",
    "            X.append(context)\n",
    "            Y.append(idx)\n",
    "            context = context[1:] + [idx]\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "def get_splits(words, valid_size=0.1, test_size=0.1):\n",
    "    n_words = len(words)\n",
    "    random.seed(42)\n",
    "    random.shuffle(words)\n",
    "\n",
    "    dev_start = int(n_words * (1 - valid_size - test_size))\n",
    "    test_start = int(n_words * (1 - test_size))\n",
    "\n",
    "    X_train, Y_train = build_dataset(words[:dev_start])\n",
    "    X_dev, Y_dev = build_dataset(words[dev_start:test_start])\n",
    "    X_test, Y_test = build_dataset(words[test_start:])\n",
    "\n",
    "    return X_train, Y_train, X_dev, Y_dev, X_test, Y_test\n",
    "\n",
    "X_train, Y_train, X_dev, Y_dev, X_test, Y_test = get_splits(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP revisited\n",
    "vocab_size = len(i2s)\n",
    "n_embeddings = 10\n",
    "n_hidden = 200\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embeddings), generator=g)\n",
    "W1 = torch.randn((n_embeddings * block_size, n_hidden), generator=g) *  5./3. / (n_embeddings * block_size) ** 0.5\n",
    "b1 = torch.randn((n_hidden,), generator=g) * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 5. / 3. / n_hidden ** 0.5\n",
    "b2 = torch.randn((vocab_size,), generator=g) * 0\n",
    "\n",
    "bn_gain = torch.ones((1, n_hidden))\n",
    "bn_bias = torch.zeros((1, n_hidden))\n",
    "\n",
    "bn_mean_running = torch.zeros((1, n_hidden))\n",
    "bn_std_running = torch.zeros((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bn_gain, bn_bias]\n",
    "print(f\"Number of params: {sum(p.nelement() for p in parameters)}\")\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    batch_idx = torch.randint(0, X_train.shape[0], (batch_size,), generator=g)\n",
    "    X_batch, Y_batch = X_train[batch_idx], Y_train[batch_idx]\n",
    "    batch_embeddings = C[X_batch]\n",
    "    embcat = batch_embeddings.view(batch_embeddings.shape[0], -1)\n",
    "    hidden_preact = embcat @ W1 + b1\n",
    "    \n",
    "    bnmean_i = hidden_preact.mean(0, keepdim=True)\n",
    "    bnstd_i = hidden_preact.std(0, keepdim=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        bn_mean_running = 0.999 * bn_mean_running + 0.001 * bnmean_i\n",
    "        bn_std_running = 0.999 * bn_std_running + 0.001 * bnstd_i\n",
    "\n",
    "    hidden_preact = bn_gain * (hidden_preact - bnmean_i) / bnstd_i + bn_bias\n",
    "    hidden_act = torch.tanh(hidden_preact)\n",
    "    logits = hidden_act @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y_batch)\n",
    "\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    lr = 0.1 if i < max_steps / 2 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # track stats\n",
    "    if i % 10000 == 0:\n",
    "        print(f\"Step {i:7d} / {max_steps:7d}, loss {loss.item():.4f}\")\n",
    "    lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_mean = hidden_preact.mean(0, keepdim=True)\n",
    "batch_std = hidden_preact.std(0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_preact.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def split_loss(dataset, split):\n",
    "    x, y = dataset[split]\n",
    "    emb = C[x]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hidden_preact = embcat @ W1 + b1\n",
    "    hidden_preact = bn_gain * (hidden_preact - bn_mean_running) / bn_std_running + bn_bias\n",
    "    hidden_act = torch.tanh(hidden_preact)\n",
    "    logits = hidden_act @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "dataset = {\"train\": (X_train, Y_train), \"dev\": (X_dev, Y_dev), \"test\": (X_test, Y_test)}\n",
    "\n",
    "split_loss(dataset, \"train\")\n",
    "split_loss(dataset, \"dev\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "n_examples = 20\n",
    "\n",
    "for _ in range(n_examples):\n",
    "    context = [0] * block_size\n",
    "    name = []\n",
    "    while True:\n",
    "        emb = C[torch.tensor(context).unsqueeze(0)]\n",
    "        embcat = emb.view(emb.shape[0], -1)\n",
    "        hidden_preact = embcat @ W1 + b1\n",
    "        hidden_preact = bn_gain * (hidden_preact - bn_mean_running) / bn_std_running + bn_bias\n",
    "        hidden_act = F.tanh(hidden_preact)\n",
    "        logits = hidden_act @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        idx = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        if idx == 0:\n",
    "            break\n",
    "        name.append(idx)\n",
    "        context = context[1:] + [idx]\n",
    "    print(\"\".join([i2s[i] for i in name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.tensor([-3.0, 5.0, 0.0, 0.0])\n",
    "logits = torch.randn((4,)) * 100\n",
    "probs = torch.softmax(logits, dim=0)\n",
    "loss = probs.log()\n",
    "probs, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_preact.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(hidden_preact.view(-1).tolist(),40);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(hidden_act.abs() > 0.99, cmap=\"gray\", interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate batch norm at the end of training\n",
    "with torch.no_grad():\n",
    "    emb = C[X_train]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    bnmean = hpreact.mean(0, keepdim=True)\n",
    "    bnstd = hpreact.std(0, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    \n",
    "    def __init__(self, fan_in, fan_out, bias: bool = True, generator=None) -> None:\n",
    "        self.weights = torch.randn((fan_in, fan_out), generator=g) / fan_in ** 0.5\n",
    "        self.bias = torch.zeros(fan_out)  if bias else None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weights\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weights, self.bias] if self.bias is not None else [self.weights]\n",
    "    \n",
    "\n",
    "class BatchNorm1d: \n",
    "    def __init__(self, dim, eps: float = 1e-5, momentum: float = 0.1) -> None:\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "\n",
    "        self.training = True\n",
    "\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)       \n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.training:\n",
    "            batch_mean = x.mean(0, keepdim=True)\n",
    "            batch_var = x.var(0, keepdim=True)\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * batch_mean\n",
    "                self.running_var = self.momentum * self.running_var + (1 - self.momentum) * batch_var\n",
    "        else:\n",
    "            batch_mean = self.running_mean\n",
    "            batch_var = self.running_var\n",
    "        \n",
    "        x_hat = (x - batch_mean) / torch.sqrt(batch_var + self.eps)\n",
    "        self.out = self.gamma * x_hat + self.beta\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "\n",
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return dout * (1 - self.out ** 2)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embeddings = 10\n",
    "n_hidden = 100\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "C = torch.randn((vocab_size, n_embeddings), generator=g)\n",
    "layers = [\n",
    "    Linear(n_embeddings * block_size, n_hidden, generator=g), BatchNorm1d(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, n_hidden, generator=g), BatchNorm1d(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, n_hidden, generator=g), BatchNorm1d(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, n_hidden, generator=g), BatchNorm1d(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, n_hidden, generator=g), BatchNorm1d(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, vocab_size, generator=g), , BatchNorm1d(vocab_size),\n",
    "]\n",
    "    \n",
    "with torch.no_grad():\n",
    "    layers[-1].weights *= 0.1\n",
    "\n",
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "\n",
    "print(f\"Number of params: {sum(p.nelement() for p in parameters)}\")\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    ix = torch.randint(0, X_train.shape[0], (batch_size,), generator=g)\n",
    "    X_batch, Y_batch = X_train[ix], Y_train[ix]\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[X_batch]\n",
    "    x = emb.view(emb.shape[0], -1)\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "    loss = F.cross_entropy(x, Y_batch)\n",
    "\n",
    "    # backward pass\n",
    "    for layer in layers:\n",
    "        layer.out.retain_grad() # !!!\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    lr = 0.1 if i < max_steps / 2 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad # type: ignore\n",
    "    \n",
    "    if i % 10000 == 0:\n",
    "        print(f\"Step {i:7d} / {max_steps:7d}, loss {loss.item():.4f}\")\n",
    "    lossi.append(loss.log10().item())\n",
    "    if i > 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for i, layer in enumerate(layers[:-1]):\n",
    "    if isinstance(layer, Tanh):\n",
    "        t = layer.out\n",
    "        print(f\"Layer {i + 1} mean: {t.mean():.4f}, std: {t.std():.4f}, saturated: {(t.abs() > 0.97).float().mean():.4f}\")\n",
    "        hy, hx = torch.histogram(t, density=True)\n",
    "        plt.plot(hx[:-1].detach(), hy.detach())\n",
    "        legends.append(f\"Layer {i + 1} {layer.__class__.__name__}\")\n",
    "plt.legend(legends)\n",
    "plt.title(\"Histogram of activations\")\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for i, layer in enumerate(layers[:-1]):\n",
    "    if isinstance(layer, Tanh):\n",
    "        t = layer.out.grad\n",
    "        print(f\"Layer {i + 1} mean: {t.mean():.4f}, std: {t.std():.4f}, saturated: {(t.abs() > 0.97).float().mean():.4f}\")\n",
    "        hy, hx = torch.histogram(t, density=True)\n",
    "        plt.plot(hx[:-1].detach(), hy.detach())\n",
    "        legends.append(f\"Layer {i + 1} {layer.__class__.__name__}\")\n",
    "plt.legend(legends)\n",
    "plt.title(\"Histogram of activations\")\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for i, p in enumerate(parameters):\n",
    "    t = p.grad\n",
    "    if p.ndim == 2:\n",
    "        print(f\"weight {t.shape} mean: {t.mean():.4f}, std: {t.std():.4f}, saturated: {(t.abs() > 0.97).float().mean():.4f}\")\n",
    "    #print(f\"Layer {i + 1} mean: {t.mean():.4f}, std: {t.std():.4f}, saturated: {(t.abs() > 0.97).float().mean():.4f}\")\n",
    "        hy, hx = torch.histogram(t, density=True)\n",
    "        plt.plot(hx[:-1].detach(), hy.detach())\n",
    "        legends.append(f\" {i + 1} {tuple(p.shape)}\")\n",
    "plt.legend(legends)\n",
    "plt.title(\"Histogram of activations\")\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
